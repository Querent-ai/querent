use candle_core::{
	quantized::{ggml_file, gguf_file},
	Device, Tensor,
};
use candle_transformers::{
	generation::{LogitsProcessor, Sampling},
	models::quantized_llama::ModelWeights,
};
use std::{io::Write, pin::Pin};
use tokenizers::Tokenizer;

use async_trait::async_trait;
use futures::Stream;

use crate::{
	language_models::{llm::LLM, options::CallOptions, GenerateResult, LLMError, TokenUsage},
	schemas::{messages::Message, StreamData},
};

use super::token_output_stream::TokenOutputStream;
use clap::{Parser, ValueEnum};

#[derive(Debug)]
pub enum Prompt {
	Interactive,
	Chat,
	One(String),
}

#[derive(Clone, Debug, Copy, PartialEq, Eq, ValueEnum)]
enum Which {
	#[value(name = "7b")]
	L7b,
	#[value(name = "13b")]
	L13b,
	#[value(name = "70b")]
	L70b,
	#[value(name = "7b-chat")]
	L7bChat,
	#[value(name = "13b-chat")]
	L13bChat,
	#[value(name = "70b-chat")]
	L70bChat,
	#[value(name = "7b-code")]
	L7bCode,
	#[value(name = "13b-code")]
	L13bCode,
	#[value(name = "32b-code")]
	L34bCode,
	#[value(name = "7b-leo")]
	Leo7b,
	#[value(name = "13b-leo")]
	Leo13b,
	#[value(name = "7b-mistral")]
	Mistral7b,
	#[value(name = "7b-mistral-instruct")]
	Mistral7bInstruct,
	#[value(name = "7b-mistral-instruct-v0.2")]
	Mistral7bInstructV02,
	#[value(name = "7b-zephyr-a")]
	Zephyr7bAlpha,
	#[value(name = "7b-zephyr-b")]
	Zephyr7bBeta,
	#[value(name = "7b-open-chat-3.5")]
	OpenChat35,
	#[value(name = "7b-starling-a")]
	Starling7bAlpha,
	#[value(name = "mixtral")]
	Mixtral,
	#[value(name = "mixtral-instruct")]
	MixtralInstruct,
	#[value(name = "llama3-8b")]
	L8b,
	#[value(name = "phi3")]
	Phi3,
}

impl Which {
	fn is_open_chat(&self) -> bool {
		match self {
			Self::L7b
			| Self::L13b
			| Self::L70b
			| Self::L7bChat
			| Self::L13bChat
			| Self::L70bChat
			| Self::L7bCode
			| Self::L13bCode
			| Self::L34bCode
			| Self::Leo7b
			| Self::Leo13b
			| Self::Mixtral
			| Self::MixtralInstruct
			| Self::Mistral7b
			| Self::Mistral7bInstruct
			| Self::Mistral7bInstructV02
			| Self::Zephyr7bAlpha
			| Self::Zephyr7bBeta
			| Self::L8b
			| Self::Phi3 => false,
			Self::OpenChat35 | Self::Starling7bAlpha => true,
		}
	}

	fn tokenizer_repo(&self) -> &'static str {
		match self {
			Self::L7b
			| Self::L13b
			| Self::L70b
			| Self::L7bChat
			| Self::L13bChat
			| Self::L70bChat
			| Self::L7bCode
			| Self::L13bCode
			| Self::L34bCode => "hf-internal-testing/llama-tokenizer",
			Self::Leo7b => "LeoLM/leo-hessianai-7b",
			Self::Leo13b => "LeoLM/leo-hessianai-13b",
			Self::Mixtral => "mistralai/Mixtral-8x7B-v0.1",
			Self::MixtralInstruct => "mistralai/Mixtral-8x7B-Instruct-v0.1",
			Self::Mistral7b
			| Self::Mistral7bInstruct
			| Self::Mistral7bInstructV02
			| Self::Zephyr7bAlpha
			| Self::Zephyr7bBeta => "mistralai/Mistral-7B-v0.1",
			Self::OpenChat35 => "openchat/openchat_3.5",
			Self::Starling7bAlpha => "berkeley-nest/Starling-LM-7B-alpha",
			Self::L8b => "meta-llama/Meta-Llama-3-8B",
			Self::Phi3 => "microsoft/Phi-3-mini-4k-instruct",
		}
	}
}

#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None)]
struct Args {
	/// GGML/GGUF file to load, typically a .bin/.gguf file generated by the quantize command from llama.cpp
	#[arg(long)]
	model: Option<String>,

	/// The initial prompt, use 'interactive' for entering multiple prompts in an interactive way
	/// and 'chat' for an interactive model where history of previous prompts and generated tokens
	/// is preserved.
	#[arg(long)]
	prompt: Option<String>,

	/// The length of the sample to generate (in tokens).
	#[arg(short = 'n', long, default_value_t = 1000)]
	sample_len: usize,

	/// The tokenizer config in json format.
	#[arg(long)]
	tokenizer: Option<String>,

	/// The temperature used to generate samples, use 0 for greedy sampling.
	#[arg(long, default_value_t = 0.8)]
	temperature: f64,

	/// Nucleus sampling probability cutoff.
	#[arg(long)]
	top_p: Option<f64>,

	/// Only sample among the top K samples.
	#[arg(long)]
	top_k: Option<usize>,

	/// The seed to use when generating random samples.
	#[arg(long, default_value_t = 299792458)]
	seed: u64,

	/// Enable tracing (generates a trace-timestamp.json file).
	#[arg(long)]
	tracing: bool,

	/// Display the token for the specified prompt.
	#[arg(long)]
	verbose_prompt: bool,

	/// Process prompt elements separately.
	#[arg(long)]
	split_prompt: bool,

	/// Run on CPU rather than GPU even if a GPU is available.
	#[arg(long)]
	cpu: bool,

	/// Penalty to be applied for repeating tokens, 1. means no penalty.
	#[arg(long, default_value_t = 1.1)]
	repeat_penalty: f32,

	/// The context size to consider for the repeat penalty.
	#[arg(long, default_value_t = 64)]
	repeat_last_n: usize,

	/// The model size to use.
	#[arg(long, default_value = "7b")]
	which: Which,

	/// Group-Query Attention, use 8 for the 70B version of LLaMAv2.
	#[arg(long)]
	gqa: Option<usize>,

	/// Use the slower dmmv cuda kernel.
	#[arg(long)]
	force_dmmv: bool,
}

impl Default for Args {
	fn default() -> Self {
		Self {
			model: None,
			prompt: None,
			sample_len: 20,
			tokenizer: None,
			temperature: 0.8,
			top_p: None,
			top_k: None,
			seed: 299792458,
			tracing: false,
			verbose_prompt: false,
			split_prompt: false,
			cpu: false,
			repeat_penalty: 1.1,
			repeat_last_n: 64,
			which: Which::L7b,
			gqa: None,
			force_dmmv: false,
		}
	}
}

impl Args {
	fn tokenizer(&self) -> anyhow::Result<Tokenizer> {
		let tokenizer_path = match &self.tokenizer {
			Some(config) => std::path::PathBuf::from(config),
			None => {
				let api = hf_hub::api::sync::Api::new()?;
				let repo = self.which.tokenizer_repo();
				let api = api.model(repo.to_string());
				api.get("tokenizer.json")?
			},
		};
		Tokenizer::from_file(tokenizer_path).map_err(anyhow::Error::msg)
	}

	fn model(&self) -> anyhow::Result<std::path::PathBuf> {
		let model_path = match &self.model {
			Some(config) => std::path::PathBuf::from(config),
			None => {
				let (repo, filename) = match self.which {
					Which::L7b => ("TheBloke/Llama-2-7B-GGML", "llama-2-7b.ggmlv3.q4_0.bin"),
					Which::L13b => ("TheBloke/Llama-2-13B-GGML", "llama-2-13b.ggmlv3.q4_0.bin"),
					Which::L70b => ("TheBloke/Llama-2-70B-GGML", "llama-2-70b.ggmlv3.q4_0.bin"),
					Which::L7bChat => {
						("TheBloke/Llama-2-7B-Chat-GGML", "llama-2-7b-chat.ggmlv3.q4_0.bin")
					},
					Which::L13bChat => {
						("TheBloke/Llama-2-13B-Chat-GGML", "llama-2-13b-chat.ggmlv3.q4_0.bin")
					},
					Which::L70bChat => {
						("TheBloke/Llama-2-70B-Chat-GGML", "llama-2-70b-chat.ggmlv3.q4_0.bin")
					},
					Which::L7bCode => ("TheBloke/CodeLlama-7B-GGUF", "codellama-7b.Q8_0.gguf"),
					Which::L13bCode => ("TheBloke/CodeLlama-13B-GGUF", "codellama-13b.Q8_0.gguf"),
					Which::L34bCode => ("TheBloke/CodeLlama-34B-GGUF", "codellama-34b.Q8_0.gguf"),
					Which::Leo7b => {
						("TheBloke/leo-hessianai-7B-GGUF", "leo-hessianai-7b.Q4_K_M.gguf")
					},
					Which::Leo13b => {
						("TheBloke/leo-hessianai-13B-GGUF", "leo-hessianai-13b.Q4_K_M.gguf")
					},
					Which::Mixtral => {
						("TheBloke/Mixtral-8x7B-v0.1-GGUF", "mixtral-8x7b-v0.1.Q4_K_M.gguf")
					},
					Which::MixtralInstruct => (
						"TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF",
						"mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf",
					),
					Which::Mistral7b => {
						("TheBloke/Mistral-7B-v0.1-GGUF", "mistral-7b-v0.1.Q4_K_S.gguf")
					},
					Which::Mistral7bInstruct => (
						"TheBloke/Mistral-7B-Instruct-v0.1-GGUF",
						"mistral-7b-instruct-v0.1.Q4_K_S.gguf",
					),
					Which::Mistral7bInstructV02 => (
						"TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
						"mistral-7b-instruct-v0.2.Q4_K_S.gguf",
					),
					Which::Zephyr7bAlpha => {
						("TheBloke/zephyr-7B-alpha-GGUF", "zephyr-7b-alpha.Q4_K_M.gguf")
					},
					Which::Zephyr7bBeta => {
						("TheBloke/zephyr-7B-beta-GGUF", "zephyr-7b-beta.Q4_K_M.gguf")
					},
					Which::OpenChat35 => ("TheBloke/openchat_3.5-GGUF", "openchat_3.5.Q4_K_M.gguf"),
					Which::Starling7bAlpha => {
						("TheBloke/Starling-LM-7B-alpha-GGUF", "starling-lm-7b-alpha.Q4_K_M.gguf")
					},
					// TODO: swap to TheBloke model when available
					Which::L8b => {
						("QuantFactory/Meta-Llama-3-8B-GGUF", "Meta-Llama-3-8B.Q4_K_S.gguf")
					},
					Which::Phi3 => {
						("microsoft/Phi-3-mini-4k-instruct-gguf", "Phi-3-mini-4k-instruct-q4.gguf")
					},
				};
				let revision = if self.which == Which::Phi3 {
					"5eef2ce24766d31909c0b269fe90c817a8f263fb"
				} else {
					"main"
				};
				let api = hf_hub::api::sync::Api::new()?;
				api.repo(hf_hub::Repo::with_revision(
					repo.to_string(),
					hf_hub::RepoType::Model,
					revision.to_string(),
				))
				.get(filename)?
			},
		};
		Ok(model_path)
	}
}

fn format_size(size_in_bytes: usize) -> String {
	if size_in_bytes < 1_000 {
		format!("{}B", size_in_bytes)
	} else if size_in_bytes < 1_000_000 {
		format!("{:.2}KB", size_in_bytes as f64 / 1e3)
	} else if size_in_bytes < 1_000_000_000 {
		format!("{:.2}MB", size_in_bytes as f64 / 1e6)
	} else {
		format!("{:.2}GB", size_in_bytes as f64 / 1e9)
	}
}

pub struct LLama {
	options: CallOptions,
	model: ModelWeights,
	tos: TokenOutputStream,
	args: Args,
}

impl LLama {
	pub fn new() -> Self {
		let args = Args::default();
		let model_path = args.model().expect("Model path not provided");
		let mut file = std::fs::File::open(&model_path).expect("Failed to open model file");
		let start = std::time::Instant::now();
		let device = device(args.cpu);
		let model = match model_path.extension().and_then(|v| v.to_str()) {
			Some("gguf") => {
				let model =
					gguf_file::Content::read(&mut file).expect("Failed to read model weights");

				let mut total_size_in_bytes = 0;
				for (_, tensor) in model.tensor_infos.iter() {
					let elem_count = tensor.shape.elem_count();
					total_size_in_bytes +=
						elem_count * tensor.ggml_dtype.type_size() / tensor.ggml_dtype.block_size();
				}
				println!(
					"loaded {:?} tensors ({}) in {:.2}s",
					model.tensor_infos.len(),
					&format_size(total_size_in_bytes),
					start.elapsed().as_secs_f32(),
				);
				ModelWeights::from_gguf(model, &mut file, &device)
					.expect("Failed to load model weights")
			},
			Some("ggml" | "bin") | Some(_) | None => {
				let model = ggml_file::Content::read(&mut file, &device)
					.expect("Failed to read model weights");
				let mut total_size_in_bytes = 0;
				for (_, tensor) in model.tensors.iter() {
					let elem_count = tensor.shape().elem_count();
					total_size_in_bytes +=
						elem_count * tensor.dtype().type_size() / tensor.dtype().block_size();
				}
				println!(
					"loaded {:?} tensors ({}) in {:.2}s",
					model.tensors.len(),
					&format_size(total_size_in_bytes),
					start.elapsed().as_secs_f32(),
				);
				println!("params: {:?}", model.hparams);
				let default_gqa = match args.which {
					Which::L7b
					| Which::L13b
					| Which::L7bChat
					| Which::L13bChat
					| Which::L7bCode
					| Which::L13bCode
					| Which::L34bCode
					| Which::Leo7b
					| Which::Leo13b
					| Which::L8b
					| Which::Phi3 => 1,
					Which::Mixtral
					| Which::MixtralInstruct
					| Which::Mistral7b
					| Which::Mistral7bInstruct
					| Which::Mistral7bInstructV02
					| Which::Zephyr7bAlpha
					| Which::Zephyr7bBeta
					| Which::L70b
					| Which::L70bChat
					| Which::OpenChat35
					| Which::Starling7bAlpha => 8,
				};
				ModelWeights::from_ggml(model, args.gqa.unwrap_or(default_gqa))
					.expect("Failed to load model weights")
			},
		};
		println!("model built");

		let tokenizer = args.tokenizer().expect("Tokenizer path not provided");
		let tos = TokenOutputStream::new(tokenizer);
		Self { options: Default::default(), model, tos, args }
	}

	pub fn with_options(mut self, options: CallOptions) -> Self {
		self.options = options;
		self
	}

	fn generate_via(&self, prompt_messages: &[Message]) -> Result<GenerateResult, LLMError> {
		let device = device(self.args.cpu);

		let prompt = prompt_messages
			.iter()
			.map(|m| m.content.clone())
			.collect::<Vec<String>>()
			.join(" ");
		let tokenizer = self.args.tokenizer().expect("Tokenizer path not provided");
		let mut tos = TokenOutputStream::new(tokenizer);
		let tokens = self
			.tos
			.tokenizer()
			.encode(prompt, true)
			.map_err(|e| LLMError::OtherError(e.to_string()))?;
		let prompt_tokens = [tokens.get_ids()].concat();
		let to_sample = self.args.sample_len.saturating_sub(1);
		let mut all_tokens = vec![];
		let mut logits_processor = {
			let temperature = self.args.temperature;
			let sampling = if temperature <= 0. {
				Sampling::ArgMax
			} else {
				match (self.args.top_k, self.args.top_p) {
					(None, None) => Sampling::All { temperature },
					(Some(k), None) => Sampling::TopK { k, temperature },
					(None, Some(p)) => Sampling::TopP { p, temperature },
					(Some(k), Some(p)) => Sampling::TopKThenTopP { k, p, temperature },
				}
			};
			LogitsProcessor::from_sampling(self.args.seed, sampling)
		};

		let start_prompt_processing = std::time::Instant::now();
		let mut generated_text = String::new();
		let mut next_token = if !self.args.split_prompt {
			let input = Tensor::new(prompt_tokens.as_slice(), &device)
				.map_err(|e| LLMError::OtherError(e.to_string()))?
				.unsqueeze(0)
				.map_err(|e| LLMError::OtherError(e.to_string()))?;
			let logits = self
				.model
				.clone()
				.forward(&input, 0)
				.map_err(|e| LLMError::OtherError(e.to_string()))?;
			let logits = logits.squeeze(0).map_err(|e| LLMError::OtherError(e.to_string()))?;
			logits_processor
				.sample(&logits)
				.map_err(|e| LLMError::OtherError(e.to_string()))?
		} else {
			let mut next_token = 0;
			for (pos, token) in prompt_tokens.iter().enumerate() {
				let input = Tensor::new(&[*token], &device)
					.map_err(|e| LLMError::OtherError(e.to_string()))?
					.unsqueeze(0)
					.map_err(|e| LLMError::OtherError(e.to_string()))?;
				let logits = self
					.model
					.clone()
					.forward(&input, pos)
					.map_err(|e| LLMError::OtherError(e.to_string()))?;
				let logits = logits.squeeze(0).map_err(|e| LLMError::OtherError(e.to_string()))?;
				next_token = logits_processor
					.sample(&logits)
					.map_err(|e| LLMError::OtherError(e.to_string()))?;
			}
			next_token
		};
		let prompt_dt = start_prompt_processing.elapsed();

		all_tokens.push(next_token);
		if let Some(t) =
			tos.next_token(next_token).map_err(|e| LLMError::OtherError(e.to_string()))?
		{
			generated_text.push_str(&t);
		}

		let eos_token = match self.args.which {
			Which::L8b => "<|end_of_text|>",
			_ => match self.args.which.is_open_chat() {
				true => "<|end_of_turn|>",
				false => "</s>",
			},
		};
		let eos_token = *tos.tokenizer().get_vocab(true).get(eos_token).unwrap();
		let start_post_prompt = std::time::Instant::now();
		let mut sampled = 0;
		for index in 0..to_sample {
			let input = Tensor::new(&[next_token], &device)
				.map_err(|e| LLMError::OtherError(e.to_string()))?
				.unsqueeze(0)
				.map_err(|e| LLMError::OtherError(e.to_string()))?;
			let logits = self
				.model
				.clone()
				.forward(&input, prompt_tokens.len() + index)
				.map_err(|e| LLMError::OtherError(e.to_string()))?;
			let logits = logits.squeeze(0).map_err(|e| LLMError::OtherError(e.to_string()))?;
			let logits = if self.args.repeat_penalty == 1. {
				logits
			} else {
				let start_at = all_tokens.len().saturating_sub(self.args.repeat_last_n);
				candle_transformers::utils::apply_repeat_penalty(
					&logits,
					self.args.repeat_penalty,
					&all_tokens[start_at..],
				)
				.map_err(|e| LLMError::OtherError(e.to_string()))?
			};
			next_token = logits_processor
				.sample(&logits)
				.map_err(|e| LLMError::OtherError(e.to_string()))?;
			all_tokens.push(next_token);
			if let Some(t) =
				tos.next_token(next_token).map_err(|e| LLMError::OtherError(e.to_string()))?
			{
				generated_text.push_str(&t);
			}
			sampled += 1;
			if next_token == eos_token {
				break;
			};
		}
		if let Some(rest) = tos.decode_rest().map_err(|e| LLMError::OtherError(e.to_string()))? {
			generated_text.push_str(&rest);
		}
		std::io::stdout().flush()?;
		let dt = start_post_prompt.elapsed();
		log::info!(
			"\n\n{:4} prompt tokens processed: {:.2} token/s",
			prompt_tokens.len(),
			prompt_tokens.len() as f64 / prompt_dt.as_secs_f64(),
		);
		log::info!(
			"{sampled:4} tokens generated: {:.2} token/s",
			sampled as f64 / dt.as_secs_f64(),
		);

		Ok(GenerateResult {
			generation: generated_text,
			tokens: Some(TokenUsage::new(prompt_tokens.len() as u32, all_tokens.len() as u32)),
		})
	}
}

pub fn device(cpu: bool) -> Device {
	if cpu {
		Device::Cpu
	} else if candle_core::utils::cuda_is_available() {
		Device::new_cuda(0).unwrap()
	} else if candle_core::utils::metal_is_available() {
		Device::new_metal(0).unwrap()
	} else {
		#[cfg(all(target_os = "macos", target_arch = "aarch64"))]
		{
			println!(
				"Running on CPU, to run on GPU(metal), build this example with `--features metal`"
			);
		}
		#[cfg(not(all(target_os = "macos", target_arch = "aarch64")))]
		{
			println!("Running on CPU, to run on GPU, build this example with `--features cuda`");
		}
		Device::Cpu
	}
}

#[async_trait]
impl LLM for LLama {
	async fn generate(&self, prompt_messages: &[Message]) -> Result<GenerateResult, LLMError> {
		self.generate_via(prompt_messages)
	}

	async fn invoke(&self, prompt: &str) -> Result<String, LLMError> {
		self.generate(&[Message::new_human_message(prompt)])
			.await
			.map(|res| res.generation)
	}

	async fn stream(
		&self,
		_messages: &[Message],
	) -> Result<Pin<Box<dyn Stream<Item = Result<StreamData, LLMError>> + Send>>, LLMError> {
		Err(LLMError::OtherError("Streaming not implemented".into()))
	}

	fn add_options(&mut self, options: CallOptions) {
		self.options.merge_options(options)
	}
}

#[cfg(test)]
mod tests {
	use super::*;

	#[test]
	fn test_llama_generate() {
		let llama = LLama::new();
		let prompt = vec![Message::new_human_message("Capital of France is ")];
		let result = llama.generate_via(&prompt);
		println!("{:?}", result);
		assert!(result.is_ok());
	}

	#[tokio::test]
	async fn test_llama_invoke() {
		let llama = LLama::new();
		let prompt = "Hello, world!";
		let result = llama.invoke(prompt).await;
		assert!(result.is_ok());
	}
}
